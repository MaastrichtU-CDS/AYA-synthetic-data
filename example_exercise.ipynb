{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Investigating the role of sample size in synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This file contains an example exercise similar to the application in the scientific publication at:\n",
    "\n",
    " \n",
    "This example was produced using a publicly available dataset on paediatric bone marrow transplantation developed by:\n",
    "\n",
    "* Marek Sikora(1,2) (marek.sikora@polsl.pl), \n",
    "* Lukasz Wrobel(1) (lukasz.wrobel@polsl.pl),  \n",
    "\n",
    "(1) Institute of Computer Science, Silesian University of Technology, 44-100 Gliwice, Poland \n",
    "(2) Institute of Innovative Technologies EMAG, 40-189 Katowice, Poland"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "    \n",
    "    \n",
    "***To ensure that this example can be run successfully, ensure that you have installed the required libraries found in requirements.txt.***\n",
    "\n",
    "***We recommend that this is done in a separate virtual environment.***\n",
    "\n",
    "***Note that this environment should be available for the Jupyter server, if not, you can opt to extract the code snippets and run it directly in a separate virtual environment.***\n",
    "\n",
    "***To install the requirements from the .txt file, use \"pip install -r requirements.txt\" in your virtual environment.***\n",
    "  \n",
    "  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To then start of, let's ensure we establish a specific directory we can work in."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# set the working directory to a folder of your desire, here we will use the example folder that is located in the \n",
    "working_directory = os.path.join(os.getcwd(), \"example_data\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First we compose a settings file, this file contains various items that relate to the various steps in the workflow.\n",
    "These items vary from specifying what pre-processing steps to take and not take, to defining a formula for logistic regression analysis.\n",
    "\n",
    "An aspect that is best not missed are the 'datatype'_corrections, which allow you to formulate a metadata file that the synthetic data generation models can use.\n",
    "For example, with biological sex being a dichotomous variable, sex should be included in boolean_corrections.\n",
    "\n",
    "*When attempting to use the code in this repository for another dataset this might require some trial and error.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"Pre-processing\": {\n",
    "        \"General_information\": {\n",
    "            \"format\": \"single_table\",\n",
    "            \"identifier\": \"\",\n",
    "            \"codebook_variable_column\": 0,\n",
    "            \"missing_value\": \"?\",\n",
    "            \"boolean_triggers\": [],\n",
    "            \"boolean_corrections\": [\n",
    "                \"Recipientgender\",\n",
    "                \"Stemcellsource\",\n",
    "                \"Donorage35\",\n",
    "                \"IIIV\",\n",
    "                \"Gendermatch\",\n",
    "                \"RecipientRh\",\n",
    "                \"ABOmatch\",\n",
    "                \"DonorCMV\",\n",
    "                \"RecipientCMV\",\n",
    "                \"Riskgroup\",\n",
    "                \"Txpostrelapse\",\n",
    "                \"Diseasegroup\",\n",
    "                \"HLAmismatch\",\n",
    "                \"Recipientage10\",\n",
    "                \"Relapse\",\n",
    "                \"aGvHDIIIIV\",\n",
    "                \"extcGvHD\",\n",
    "                \"survival_status\"\n",
    "            ],\n",
    "            \"categorical_variables\": [],\n",
    "            \"categorical_corrections\": [\n",
    "                \"DonorABO\",\n",
    "                \"RecipientABO\",\n",
    "                \"CMVstatus\",\n",
    "                \"Disease\",\n",
    "                \"HLAmatch\",\n",
    "                \"Antigen\",\n",
    "                \"Alel\",\n",
    "                \"HLAgrI\",\n",
    "                \"Recipientageint\"\n",
    "            ],\n",
    "            \"float_triggers\": [],\n",
    "            \"float_corrections\": [\"CD34kgx10d6\", \"CD3dCD34\", \"CD3dkgx10d8\", \"Rbodymass\"],\n",
    "            \"integer_triggers\": [],\n",
    "            \"integer_corrections\": [\n",
    "                \"Donorage\",\n",
    "                \"Recipientage\",\n",
    "                \"ANCrecovery\",\n",
    "                \"PLTrecovery\",\n",
    "                \"time_to_aGvHD_III_IV\",\n",
    "                \"survival_time\"\n",
    "            ],\n",
    "            \"string_triggers\": [],\n",
    "            \"string_corrections\": []\n",
    "        },\n",
    "        \"Settings\": {\n",
    "            \"harmonise_booleans\": \"False\",\n",
    "            \"find_codebook_discrepancies\": \"True\",\n",
    "            \"recode_categories\": [\n",
    "                \"Disease\"\n",
    "            ],\n",
    "            \"remove_free_text\": \"False\",\n",
    "            \"remove_identification\": \"False\",\n",
    "            \"remove_in_column_string\": {},\n",
    "            \"remove_high_percentage_missing\": [],\n",
    "            \"variables_to_keep\": []\n",
    "        }\n",
    "    },\n",
    "    \"Evaluation\": {\n",
    "        \"formula_logistic_regression\": \"\",\n",
    "        \"variables_to_plot\": [],\n",
    "        \"variables_nickname\": {},\n",
    "        \"known_variables\": ['Recipientgender', 'Stemcellsource'],\n",
    "        \"sensitive_variables\": ['Donorage'],\n",
    "        \"graph_file_extension\": \".eps\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We then save the settings file so that it can be re-used in subsequent analyses. \n",
    "\n",
    "In case you already have a settings file that is appropriately formatted, it is not necessary to save it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from src.file_handling import save_json\n",
    "\n",
    "# save the file as name_settings.json, as this is the format that will be sought for later in the workflow; this name (bmt) refers to bone marrow transplantation\n",
    "settings_name = \"bmt_settings.json\"\n",
    "save_json(data=settings, filename=os.path.join(working_directory, \"bmt_settings.json\"))\n",
    "\n",
    "# the settings file is now saved in the location that is findable through\n",
    "settings_path = os.path.join(working_directory, \"bmt_settings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "With the settings saved, we can now start pre-processing the data, this is done by providing the path to the dataset and the path to the settings file to the pre-processing component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from src.data_pre_processing import UnProcessedData\n",
    "\n",
    "# formulate the path to the dataset, in this example the dataset is called bmt.csv and is located in the same folder as this example workflow\n",
    "dataset_path = os.path.join(working_directory, \"bmt.csv\")\n",
    "\n",
    "# provide that path to the dataset and the previously defined path to the settings to the pre-processing component; we provide an empty string for the codebook, as this is not available here\n",
    "pre_processed_data = UnProcessedData(data_path=dataset_path, codebook_path=\"\", settings_path=settings_path)\n",
    "\n",
    "# clean the data, and save it as name_clean.csv; this will be saved in the same folder as your dataset path\n",
    "pre_processed_data.clean_data(save=True, filename_addition='_clean')\n",
    "\n",
    "# formulate and save a Synthetic Data Vault metadata file; this will be saved in same folder as your dataset path\n",
    "pre_processed_data.format_metadata(save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In your working directory you now should have the following files\n",
    " * original CSV dataset, \n",
    " * a clean CSV dataset, \n",
    " * a JSON file containing the SDV metadata, and \n",
    " * a JSON file containing the settings.\n",
    "\n",
    "In case you've been using the provided example, your example_data folder should have the following files:\n",
    "\n",
    "![Image of files in your working directory](assests_for_jupyter_example/files_in_your_working_directory_pre.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Synthetic data generation and evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation of output sample size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Due to the nature of the variation in sample sizes (i.e., the variation in training and synthetic data sample size), the process of modelling a dataset, generating synthetic data, and the evaluation thereof, has been bundled in a single pipeline.\n",
    "*However, the functions are generally independently callable if this is desired.*\n",
    "\n",
    "For the sake of completeness, what will happen is the following:\n",
    "* the dataset is modelled using a generative model,\n",
    "* this generative model then produces synthetic datasets with sizes as specified,\n",
    "* these synthetic datasets are then compared to the dataset that was used to train the generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from src.evaluation_general import DataEvaluation\n",
    "\n",
    "# specify the model that you wish to evaluate; note that only single table models are supported in our pipeline, which are: FAST_ML, GaussianCopula, CopulaGan, CTGAN, and TVAE; DP-CGAN is available in its specific branch\n",
    "# for this example we will use DP-CGAN, as this is the DP-CGAN specific branch\n",
    "generative_model = \"DP-CGAN\"\n",
    "\n",
    "# we can also define the range that we produce synthetic data for, including the interval (i.e., the output sample size)\n",
    "smallest_sample_size = 50\n",
    "largest_sample_size = 1000\n",
    "sample_size_interval = 100\n",
    "\n",
    "# we now only have to specific the working directory, given that all files are present as described above\n",
    "evaluator = DataEvaluation(directory=working_directory)\n",
    "\n",
    "# now we specify the evaluation, there are multiple options available, for this example we will only evaluate output sample size; we include a variable default model to ensure that it uses default (hyper-)parameters\n",
    "evaluator.generator_evaluate_n_output(start=smallest_sample_size, stop=largest_sample_size, step=sample_size_interval,\n",
    "                                      models_to_evaluate=[generative_model], default_model=True, save_data=True,\n",
    "                                      save_model=True)\n",
    "\n",
    "# we remove the object again for the purpose of this example\n",
    "del evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In your working directory, you should now have the following new directories, and files\n",
    "* evaluation\n",
    "    * dump\n",
    "    * name_clean_n_output.csv\n",
    "    * name_clean_n_output_data_quality.eps\n",
    "* models\n",
    "    * name_clean_n_output_model_model_name.pk1 \n",
    "* synthetic\n",
    "    * name_clean_model_name_sample_size_evaluation_n_output_file_id.csv\n",
    "\n",
    "In case you’ve been using the provided example, your example_data folder should have the following files *with the exception for the specific file names of the generated data*:\n",
    "![Image of files in your working directory](assests_for_jupyter_example/files_in_your_working_directory_post.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Additionally, a figure has been produced by this process and is displayed in your Jupyter notebook; *in case it is not visible you should be able to find it in the evaluation folder*.\n",
    "This figure represents a trade-off between:\n",
    "* veracity in the horizontal lines (in this case through the precision, recall, density, and coverage metrics) and \n",
    "* privacy concealment in the vertical lines (in this case through the identity disclosure metric).\n",
    "\n",
    "In case you've been using the provided example, your figure should more or less look as follows: \n",
    "*please do note that the results will never exactly be the same due to the re-modelling of data*\n",
    "![Image produced by evaluating the synthetic data sample size with DP-CGAN](assests_for_jupyter_example/bmt_clean_n_output_data_quality_DP-CGAN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This concludes the example of evaluating output sample size, to finish up, we can move the results of our evaluation to a directory specific to the generative model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.move(os.path.join(working_directory, \"evaluation\"),\n",
    "            os.path.join(working_directory, generative_model, \"evaluation\"))\n",
    "shutil.move(os.path.join(working_directory, \"models\"),\n",
    "            os.path.join(working_directory, generative_model, \"models\"))\n",
    "shutil.move(os.path.join(working_directory, \"synthetic\"),\n",
    "            os.path.join(working_directory, generative_model, \"synthetic\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation of input and output sample size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Due to the nature of the variation in sample sizes (i.e., the variation in training and synthetic data sample size), the process of modelling a dataset, generating synthetic data, and the evaluation thereof, has been bundled in a single pipeline.\n",
    "*However, the functions are generally independently callable if this is desired.*\n",
    "\n",
    "For the sake of completeness, what will happen in this scenario is the following:\n",
    "* the entire dataset is modelled using a generative model,\n",
    "* this generative model then produces synthetic datasets with sizes as specified,\n",
    "* these synthetic datasets are then compared to the dataset that was used to train the generative model,\n",
    "* the entire dataset is sub-sampled, this sub-sample is then modelled using a new generative model,\n",
    "* this new generative model then produces synthetic datasets with sizes as specified,\n",
    "* these synthetic datasets are then compared to the dataset that was used to train the generative model (in this case thus a sub-sample),\n",
    "* the entire dataset is sub-sampled again, et cetera..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from src.evaluation_general import DataEvaluation\n",
    "\n",
    "# specify the model that you wish to evaluate; note that only single table models are supported in our pipeline, which are: FAST_ML, GaussianCopula, CopulaGan, CTGAN, and TVAE; DP-CGAN is available in its specific branch\n",
    "# for this example we will use DP-CGAN, as this is the DP-CGAN specific branch\n",
    "generative_model = \"DP-CGAN\"\n",
    "\n",
    "# additionally we can define the smallest sample that we which to draw from the entire dataset (i.e., the input/training sample size)\n",
    "smallest_input_sample_size = 80\n",
    "sample_size_interval = 50\n",
    "\n",
    "# we can also define the range that we produce synthetic data for, including the interval (i.e., the output sample size)\n",
    "smallest_output_sample_size = 50\n",
    "largest_output_sample_size = 1000\n",
    "sample_output_size_interval = 100\n",
    "\n",
    "# we now only have to specific the working directory, given that all files are present as described above\n",
    "evaluator = DataEvaluation(directory=working_directory)\n",
    "\n",
    "# now we specify the evaluation, there are multiple options available, for this example we will only evaluate output sample size; we include a variable default model to ensure that it uses default (hyper-)parameters\n",
    "# we're avoiding saving any data here, as this can quickly consume a lot of disk space\n",
    "evaluator.generator_evaluate_n_input_random(stop=smallest_input_sample_size, step=sample_size_interval,\n",
    "                                            output_start=smallest_output_sample_size,\n",
    "                                            output_stop=largest_output_sample_size,\n",
    "                                            output_step=sample_output_size_interval,\n",
    "                                            models_to_evaluate=[generative_model], default_model=True)\n",
    "\n",
    "# we remove the object again for the purpose of this example\n",
    "del evaluator"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In your working directory, you should now have the following new directories, and files\n",
    "* evaluation\n",
    "    * dump\n",
    "    * name_clean_n_input_sample_size_n_output.csv\n",
    "    * name_clean_n_input_sample_size_n_output.eps\n",
    "* models\n",
    "* synthetic\n",
    "\n",
    "In case you’ve been using the provided example, your example_data folder should have the following files *with the exception for the specific file names of the generated data*:\n",
    "![Image of files in your working directory when performing input evaluation](assests_for_jupyter_example/files_in_your_working_directory_input_evaluation.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This concludes the example exercise of this research code, the results that have been stored in the csv files can be used to produce figures similar to those in the aforementioned scientific publication. Interpretation of the results in this exercise was considered to go beyond the scope of this document."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
